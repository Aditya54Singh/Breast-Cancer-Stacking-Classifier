# Breast-Cancer-Stacking-Classifier


This repository contains a Jupyter Notebook (`Stacking.ipynb`) that demonstrates the implementation of a stacking ensemble classifier for breast cancer prediction. The project utilizes the `Breast Cancer Wisconsin (Diagnostic)` dataset, a well-known dataset for binary classification tasks, to predict whether a tumor is malignant or benign.

## Table of Contents

- [Introduction](#introduction)
- [Dataset](#dataset)
- [Methodology](#methodology)
  - [Base Models](#base-models)
  - [Meta-Model](#meta-model)
  - [Stacking Process](#stacking-process)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Dependencies](#dependencies)

## Introduction

Ensemble learning techniques combine multiple individual models to achieve better predictive performance than any single model could achieve alone. Stacking is an advanced ensemble method that trains a "meta-model" to combine the predictions of several "base models." This project applies stacking to a binary classification problem using the breast cancer dataset, aiming to improve classification accuracy and robustness.

## Dataset

The model is trained and evaluated using the `Breast Cancer Wisconsin (Diagnostic)` dataset, which is readily available through scikit-learn's `load_breast_cancer` function. This dataset contains features computed from digitized images of a fine needle aspirate (FNA) of a breast mass, describing characteristics of the cell nuclei present in the image. The target variable indicates whether the tumor is malignant (0) or benign (1).

The dataset includes 30 features, such as:
- `mean radius`
- `mean texture`
- `mean perimeter`
- `mean area`
- `mean smoothness`
- ...and more, including various error and "worst" (mean of the three largest values) features.

## Methodology

The core of this project involves building and evaluating a stacking classifier. The process includes data splitting, training individual base models, and then training a meta-model on the predictions of these base models.

### Base Models

Four different machine learning algorithms were used as base models:
1.  **Logistic Regression** (`LogisticRegression`)
2.  **Decision Tree Classifier** (`DecisionTreeClassifier`)
3.  **Support Vector Classifier** (`SVC`)
4.  **K-Nearest Neighbors Classifier** (`KNeighborsClassifier`)

Each base model is encapsulated within a `Pipeline` that first applies `StandardScaler` for feature scaling, which is crucial for algorithms sensitive to feature magnitudes like Logistic Regression, SVC, and KNN.

### Meta-Model

A **Logistic Regression** model is used as the `final_estimator` (meta-model) in the `StackingClassifier`. This meta-model learns to optimally combine the predictions (or probabilities) generated by the base models.

### Stacking Process

The `StackingClassifier` is configured with:
- `estimators`: A list of the base models (`lr`, `dt`, `svc`, `knn`).
- `final_estimator`: The meta-model (`LogisticRegression`).
- `cv`: 5-fold cross-validation is used for training the meta-model, ensuring robust generalization.
- `passthrough=False`: This indicates that only the predictions of the base models are fed to the meta-model, not the original features.

The stacked model is trained on the training data (`x_train`, `y_train`) after the data has been split into training and testing sets with a `test_size` of 0.2 and `random_state=42`.

## Results

The accuracy of the individual base models and the stacked ensemble model on the test set are as follows:

| Model                 | Accuracy | Precision (Malignant) | Recall (Malignant) | F1-Score (Malignant) | Precision (Benign) | Recall (Benign) | F1-Score (Benign) |
|-----------------------|----------|-----------------------|--------------------|----------------------|--------------------|-----------------|-------------------|
| Logistic Regression   | 0.974    | 0.98                  | 0.95               | 0.96                 | 0.97               | 0.99            | 0.98              |
| Decision Tree         | 0.947    | 0.93                  | 0.93               | 0.93                 | 0.96               | 0.96            | 0.96              |
| SVC                   | 0.982    | 1.00                  | 0.95               | 0.98                 | 0.97               | 1.00            | 0.99              |
| K-Nearest Neighbors   | 0.947    | 0.93                  | 0.93               | 0.93                 | 0.96               | 0.96            | 0.96              |
